{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Idea in GANs - \n",
      "\n",
      "Two probability distribution -\n",
      "Pr - Distribution from real images\n",
      "Pg - Distribution that comes out from the generator implicit distribution\n",
      "\n",
      "\n",
      "Here we want these distribution Pg, Pr to be very similar in order to generate realistic looking images\n",
      "\n",
      "\n",
      "Q - How we define a distance between two probability distribution Pg, Pr\n",
      "\n",
      "-> There are some comman ways from statistics\n",
      "\n",
      "1) Kullback-leibler (KL) divergence\n",
      "2) Jensen-Shannon (JS) divergence\n",
      "3) Wasserstein Distance\n",
      "\n",
      "Formula - \n",
      "\n",
      "max Ex~Pr[f(x)] - Ex~Pthita[f(x)] ||f||L<=1\n",
      "        |               |             |  \n",
      "    Real Data       Generator   Constraint on discriminator\n",
      "\n",
      "\n",
      "Discriminator wants to seperate these as much as possible (maximize loss)\n",
      "\n",
      "Generator wants to put these as much closer to each other (Minimize)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\"\n",
    "Idea in GANs - \n",
    "\n",
    "Two probability distribution -\n",
    "Pr - Distribution from real images\n",
    "Pg - Distribution that comes out from the generator implicit distribution\n",
    "\n",
    "\n",
    "Here we want these distribution Pg, Pr to be very similar in order to generate realistic looking images\n",
    "\n",
    "\n",
    "Q - How we define a distance between two probability distribution Pg, Pr\n",
    "\n",
    "-> There are some comman ways from statistics\n",
    "\n",
    "1) Kullback-leibler (KL) divergence\n",
    "2) Jensen-Shannon (JS) divergence\n",
    "3) Wasserstein Distance\n",
    "\n",
    "Formula - \n",
    "\n",
    "max Ex~Pr[f(x)] - Ex~Pthita[f(x)] ||f||L<=1\n",
    "        |               |             |  \n",
    "    Real Data       Generator   Constraint on discriminator\n",
    "\n",
    "\n",
    "Discriminator wants to seperate these as much as possible (maximize loss)\n",
    "\n",
    "Generator wants to put these as much closer to each other (Minimize)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary PyTorch modules for building neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Discriminator class that inherits from nn.Module\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        # Call the __init__ method of nn.Module (the parent class)\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define the sequential model (a linear stack of layers) for the discriminator network\n",
    "        self.disc = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            # Input shape: (N, channels_img, 64, 64), where N is the batch size, channels_img is the number of input channels\n",
    "            nn.Conv2d(\n",
    "                channels_img,  # Number of input channels (e.g., 3 for RGB images)\n",
    "                features_d,     # Number of output channels (depth) for this layer (controls the feature maps)\n",
    "                kernel_size=4,  # Size of the convolutional filter (4x4)\n",
    "                stride=2,       # How much the filter moves across the image (2 pixels at a time)\n",
    "                padding=1       # Padding added around the image to maintain size after convolution\n",
    "            ),\n",
    "            # Apply LeakyReLU activation to add non-linearity\n",
    "            # LeakyReLU allows a small, non-zero gradient when the unit is not active\n",
    "            nn.LeakyReLU(0.2),  # Negative slope of 0.2 to avoid vanishing gradients for negative values\n",
    "\n",
    "            # Second block: Convolution + BatchNorm + LeakyReLU\n",
    "            # This block will downsample the image and learn more complex features\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),  # Increases depth (number of feature maps)\n",
    "            \n",
    "            # Third block: Same structure with more filters (features_d * 2 to features_d * 4)\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            \n",
    "            # Fourth block: Same structure with even more filters (features_d * 4 to features_d * 8)\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            \n",
    "            # Final block: Convolution to reduce the output to a single value (the \"real\" or \"fake\" prediction)\n",
    "            # Output shape will be (N, 1, 1, 1), where N is the batch size, 1 represents the final output\n",
    "            nn.Conv2d(\n",
    "                features_d * 8,  # Input depth (number of feature maps)\n",
    "                1,               # Output depth is 1 (single prediction for real or fake)\n",
    "                kernel_size=4,    # 4x4 filter\n",
    "                stride=2,         # Stride of 2 for downsampling\n",
    "                padding=1         # Padding to ensure proper dimensionality\n",
    "            ),\n",
    "            \n",
    "            # Sigmoid activation function: It squashes the output between 0 and 1\n",
    "            # This is useful for binary classification (real or fake)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # A helper method to create a block of Conv2D -> BatchNorm -> LeakyReLU layers\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            # Convolutional layer\n",
    "            nn.Conv2d(\n",
    "                in_channels,   # Input depth (from the previous layer)\n",
    "                out_channels,  # Output depth (number of feature maps this layer produces)\n",
    "                kernel_size,   # Filter size (e.g., 4x4)\n",
    "                stride,        # Stride for downsampling\n",
    "                padding,       # Padding to keep the image size consistent\n",
    "                bias=False     # No bias term as BatchNorm takes care of it\n",
    "            ),\n",
    "            # Batch normalization helps stabilize training by normalizing the output of the Conv layer\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # Apply LeakyReLU activation function\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    # Forward method: Defines how the input tensor (image) flows through the network layers\n",
    "    def forward(self, x):\n",
    "        # Pass the input 'x' (image) through the 'disc' network (the sequential layers)\n",
    "        return self.disc(x)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ______________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "    print(\"Success, tests passed!\")\n",
    "\n",
    "\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m\n\u001b[0;32m     58\u001b[0m disc\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Target labels not needed! <3 unsupervised\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (real, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     63\u001b[0m         real \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     64\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(BATCH_SIZE, NOISE_DIM, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training of DCGAN network on MNIST dataset with Discriminator\n",
    "and Generator imported from models.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# If you train on MNIST, remember to set channels_img to 1\n",
    "dataset = r\"C:\\Users\\Gaurav\\OneDrive\\Desktop\\Text2Image\\data\\Extracted\\archive (4)\\flickr30k_images\\flickr30k_images\"\n",
    "\n",
    "\n",
    "# comment mnist above and uncomment below if train on CelebA\n",
    "# dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
